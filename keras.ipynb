{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow , Dtype = <class 'numpy.ndarray'>, Total values in Set:21542\n",
      "obs_tas , Dtype = <class 'numpy.ndarray'>, Total values in Set:21546\n",
      "flow_dates , Dtype = <class 'pandas.core.series.Series'>, Total values in Set:21542\n",
      "obs_lon_lat , Dtype = <class 'list'>, Total values in Set:2\n",
      "obs_pr , Dtype = <class 'numpy.ndarray'>, Total values in Set:21546\n",
      "obs_dates , Dtype = <class 'pandas.core.series.Series'>, Total values in Set:21546\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "### Read the dataFile \n",
    "Read = pickle.load( open( \"uke_eggedal_data_challenge.pkl\", \"rb\" ) )\n",
    "\n",
    "#save data on my variable\n",
    "data  = {}\n",
    "for ids,sets in Read.items():\n",
    "    data[ids] = sets\n",
    "\n",
    "#Check keys and data types in the  dic\n",
    "for ids,sets in data.items():\n",
    "    print(ids, f', Dtype = {type(sets)}, Total values in Set:{len(sets)}')\n",
    "    \n",
    "#Separate the obs_tas and obs_pr arrays into individual columns to make things easier to see\n",
    "Temp = {}\n",
    "precipitation ={}\n",
    "\n",
    "#create dummy data for dictionary depending on lenght of values \n",
    "for i in data['obs_pr']:\n",
    "    Length = len(i)\n",
    "    for j in range (Length):\n",
    "        precipitation[f'Prec_Zone_{j}'] = []\n",
    "#Fill the empty list with the corresponding data \n",
    "index = 0\n",
    "for ids,val in precipitation.items():\n",
    "    for i in data['obs_pr']:\n",
    "        precipitation[ids].insert(index,i[index])\n",
    "    index = index+1    \n",
    "dfprecipitation = pd.DataFrame(precipitation)\n",
    "\n",
    "# Repeat for Temperature\n",
    "\n",
    "#create dummy data for dictionary depending on lenght of values \n",
    "for i in data['obs_tas']:\n",
    "    Length = len(i)\n",
    "    for j in range (Length):\n",
    "        Temp[f'Temp_Zone_{j}'] = []\n",
    "#Fill the empty list with the corresponding data \n",
    "index = 0\n",
    "for ids,val in Temp.items():\n",
    "    for i in data['obs_tas']:\n",
    "        Temp[ids].insert(index,i[index])\n",
    "    index = index+1  \n",
    "dfTemp = pd.DataFrame(Temp)\n",
    "#add the date to each DF \n",
    "dfTemp['Date'] = data['obs_dates'].reset_index(drop=True) # Reset Index to keep same index as initial DF\n",
    "dfTemp['Date'] = pd.to_datetime(dfTemp['Date'])\n",
    "dfprecipitation['Date'] = data['obs_dates'].reset_index(drop=True) # Reset Index to keep same index as initial DF\n",
    "dfprecipitation['Date'] = pd.to_datetime(dfprecipitation['Date'])\n",
    "# Create new DataFrame for all data together\n",
    "DfTemp_Prec = dfprecipitation.merge(dfTemp, how='inner', on='Date')\n",
    "\n",
    "#Create DataFrame for de Flow data\n",
    "dfFlow = pd.DataFrame(data['flow'],columns = ['Flow'])\n",
    "dfFlow['Date'] = data['flow_dates'].reset_index(drop=True)\n",
    "#Now we merge the two datasets where the dates are the same \n",
    "Df_Final= DfTemp_Prec.merge(dfFlow, how='inner', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "raw_seq = Df_Final['Flow']\n",
    "# choose a number of time steps\n",
    "n_steps = 20\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd687feca0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalTest = Df_Final[(Df_Final['Date'] >= f'2015-01-02')]\n",
    "Original= DfTemp_Prec.merge(dfFlow, how='inner', on='Date')\n",
    "Original = Original[(Original['Date'] >= f'2015-01-02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "XFinal = pd.DataFrame(columns=['Flow'])\n",
    "XFinal['Flow'] = Original['Flow'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4755273]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = np.array(XFinal['Flow'])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20814    1.522931\n",
       "20815    0.364669\n",
       "20816    2.190361\n",
       "20817    2.729620\n",
       "20818    1.018141\n",
       "20819    2.727857\n",
       "20820    1.709606\n",
       "20821    0.107957\n",
       "20822    2.686432\n",
       "20823    1.372319\n",
       "20824    4.118482\n",
       "20825    3.260375\n",
       "20826    2.173459\n",
       "20827    2.351025\n",
       "20828    1.312288\n",
       "20829    0.132999\n",
       "20830    1.909995\n",
       "20831    3.441909\n",
       "20832    1.721519\n",
       "20833    0.001000\n",
       "20834    1.465748\n",
       "Name: Flow, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Original['Flow'][:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dany_\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\dany_\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: kerasModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('kerasModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
